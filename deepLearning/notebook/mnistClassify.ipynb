{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 读取mnist 数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.MNIST'>\n",
      "torch.Size([10000])\n",
      "torch.Size([60000, 784])\n",
      "torch.Size([60000, 784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = torchvision.datasets.MNIST(\"../data/\", train=True, download=True,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                       torchvision.transforms.ToTensor(),\n",
    "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                   ]))\n",
    "test = torchvision.datasets.MNIST(\"../data/\", train=False, download=True,\n",
    "                                  transform=torchvision.transforms.Compose([\n",
    "                                      torchvision.transforms.ToTensor(),\n",
    "                                      torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                  ]))\n",
    "print(type(train))\n",
    "train.data = train.data.reshape((-1, 28 * 28))\n",
    "test.data = test.data.reshape((-1, 28 * 28))\n",
    "print(train.data.shape)\n",
    "print(train.data.shape)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0: loss=-0.10071037709712982, acc=0.078125\n",
      "step:1: loss=-0.12499997764825821, acc=0.125\n",
      "step:2: loss=-0.09375, acc=0.09375\n",
      "step:3: loss=-0.125, acc=0.125\n",
      "step:4: loss=-0.0625, acc=0.0625\n",
      "step:5: loss=-0.0625, acc=0.0625\n",
      "step:6: loss=-0.09375, acc=0.09375\n",
      "step:7: loss=-0.03125, acc=0.03125\n",
      "step:8: loss=-0.09375, acc=0.09375\n",
      "step:9: loss=-0.140625, acc=0.140625\n",
      "step:10: loss=-0.15625, acc=0.15625\n",
      "step:11: loss=-0.1875, acc=0.1875\n",
      "step:12: loss=-0.078125, acc=0.078125\n",
      "step:13: loss=-0.078125, acc=0.078125\n",
      "step:14: loss=-0.109375, acc=0.109375\n",
      "step:15: loss=-0.046875, acc=0.046875\n",
      "step:16: loss=-0.109375, acc=0.109375\n",
      "step:17: loss=-0.15625, acc=0.15625\n",
      "step:18: loss=-0.125, acc=0.125\n",
      "step:19: loss=-0.171875, acc=0.171875\n",
      "step:20: loss=-0.09375, acc=0.09375\n",
      "step:21: loss=-0.078125, acc=0.078125\n",
      "step:22: loss=-0.125, acc=0.125\n",
      "step:23: loss=-0.140625, acc=0.140625\n",
      "step:24: loss=-0.09375, acc=0.09375\n",
      "step:25: loss=-0.109375, acc=0.109375\n",
      "step:26: loss=-0.09375, acc=0.09375\n",
      "step:27: loss=-0.125, acc=0.125\n",
      "step:28: loss=-0.078125, acc=0.078125\n",
      "step:29: loss=-0.09375, acc=0.09375\n",
      "step:30: loss=-0.09375, acc=0.09375\n",
      "step:31: loss=-0.109375, acc=0.109375\n",
      "step:32: loss=-0.15625, acc=0.15625\n",
      "step:33: loss=-0.09375, acc=0.09375\n",
      "step:34: loss=-0.171875, acc=0.171875\n",
      "step:35: loss=-0.171875, acc=0.171875\n",
      "step:36: loss=-0.1875, acc=0.1875\n",
      "step:37: loss=-0.171875, acc=0.171875\n",
      "step:38: loss=-0.078125, acc=0.078125\n",
      "step:39: loss=-0.078125, acc=0.078125\n",
      "step:40: loss=-0.140625, acc=0.140625\n",
      "step:41: loss=-0.078125, acc=0.078125\n",
      "step:42: loss=-0.09375, acc=0.09375\n",
      "step:43: loss=-0.078125, acc=0.078125\n",
      "step:44: loss=-0.140625, acc=0.140625\n",
      "step:45: loss=-0.09375, acc=0.09375\n",
      "step:46: loss=-0.046875, acc=0.046875\n",
      "step:47: loss=-0.09375, acc=0.09375\n",
      "step:48: loss=-0.109375, acc=0.109375\n",
      "step:49: loss=-0.09375, acc=0.09375\n",
      "step:50: loss=-0.109375, acc=0.109375\n",
      "step:51: loss=-0.140625, acc=0.140625\n",
      "step:52: loss=-0.078125, acc=0.078125\n",
      "step:53: loss=-0.09375, acc=0.09375\n",
      "step:54: loss=-0.078125, acc=0.078125\n",
      "step:55: loss=-0.125, acc=0.125\n",
      "step:56: loss=-0.15625, acc=0.15625\n",
      "step:57: loss=-0.0625, acc=0.0625\n",
      "step:58: loss=-0.109375, acc=0.109375\n",
      "step:59: loss=-0.140625, acc=0.140625\n",
      "step:60: loss=-0.125, acc=0.125\n",
      "step:61: loss=-0.0625, acc=0.0625\n",
      "step:62: loss=-0.15625, acc=0.15625\n",
      "step:63: loss=-0.09375, acc=0.09375\n",
      "step:64: loss=-0.109375, acc=0.109375\n",
      "step:65: loss=-0.109375, acc=0.109375\n",
      "step:66: loss=-0.078125, acc=0.078125\n",
      "step:67: loss=-0.125, acc=0.125\n",
      "step:68: loss=-0.09375, acc=0.09375\n",
      "step:69: loss=-0.109375, acc=0.109375\n",
      "step:70: loss=-0.171875, acc=0.171875\n",
      "step:71: loss=-0.109375, acc=0.109375\n",
      "step:72: loss=-0.09375, acc=0.09375\n",
      "step:73: loss=-0.09375, acc=0.09375\n",
      "step:74: loss=-0.125, acc=0.125\n",
      "step:75: loss=-0.125, acc=0.125\n",
      "step:76: loss=-0.078125, acc=0.078125\n",
      "step:77: loss=-0.15625, acc=0.15625\n",
      "step:78: loss=-0.125, acc=0.125\n",
      "step:79: loss=-0.109375, acc=0.109375\n",
      "step:80: loss=-0.09375, acc=0.09375\n",
      "step:81: loss=-0.140625, acc=0.140625\n",
      "step:82: loss=-0.078125, acc=0.078125\n",
      "step:83: loss=-0.078125, acc=0.078125\n",
      "step:84: loss=-0.078125, acc=0.078125\n",
      "step:85: loss=-0.125, acc=0.125\n",
      "step:86: loss=-0.140625, acc=0.140625\n",
      "step:87: loss=-0.140625, acc=0.140625\n",
      "step:88: loss=-0.078125, acc=0.078125\n",
      "step:89: loss=-0.015625, acc=0.015625\n",
      "step:90: loss=-0.21875, acc=0.21875\n",
      "step:91: loss=-0.078125, acc=0.078125\n",
      "step:92: loss=-0.171875, acc=0.171875\n",
      "step:93: loss=-0.15625, acc=0.15625\n",
      "step:94: loss=-0.109375, acc=0.109375\n",
      "step:95: loss=-0.109375, acc=0.109375\n",
      "step:96: loss=-0.109375, acc=0.109375\n",
      "step:97: loss=-0.109375, acc=0.109375\n",
      "step:98: loss=-0.203125, acc=0.203125\n",
      "step:99: loss=-0.109375, acc=0.109375\n",
      "step:100: loss=-0.109375, acc=0.109375\n",
      "step:101: loss=-0.15625, acc=0.15625\n",
      "step:102: loss=-0.125, acc=0.125\n",
      "step:103: loss=-0.1875, acc=0.1875\n",
      "step:104: loss=-0.140625, acc=0.140625\n",
      "step:105: loss=-0.078125, acc=0.078125\n",
      "step:106: loss=-0.109375, acc=0.109375\n",
      "step:107: loss=-0.0625, acc=0.0625\n",
      "step:108: loss=-0.0625, acc=0.0625\n",
      "step:109: loss=-0.125, acc=0.125\n",
      "step:110: loss=-0.078125, acc=0.078125\n",
      "step:111: loss=-0.109375, acc=0.109375\n",
      "step:112: loss=-0.09375, acc=0.09375\n",
      "step:113: loss=-0.125, acc=0.125\n",
      "step:114: loss=-0.015625, acc=0.015625\n",
      "step:115: loss=-0.140625, acc=0.140625\n",
      "step:116: loss=-0.1875, acc=0.1875\n",
      "step:117: loss=-0.15625, acc=0.15625\n",
      "step:118: loss=-0.203125, acc=0.203125\n",
      "step:119: loss=-0.15625, acc=0.15625\n",
      "step:120: loss=-0.078125, acc=0.078125\n",
      "step:121: loss=-0.078125, acc=0.078125\n",
      "step:122: loss=-0.125, acc=0.125\n",
      "step:123: loss=-0.15625, acc=0.15625\n",
      "step:124: loss=-0.0625, acc=0.0625\n",
      "step:125: loss=-0.140625, acc=0.140625\n",
      "step:126: loss=-0.109375, acc=0.109375\n",
      "step:127: loss=-0.140625, acc=0.140625\n",
      "step:128: loss=-0.109375, acc=0.109375\n",
      "step:129: loss=-0.078125, acc=0.078125\n",
      "step:130: loss=-0.125, acc=0.125\n",
      "step:131: loss=-0.21875, acc=0.21875\n",
      "step:132: loss=-0.109375, acc=0.109375\n",
      "step:133: loss=-0.109375, acc=0.109375\n",
      "step:134: loss=-0.171875, acc=0.171875\n",
      "step:135: loss=-0.140625, acc=0.140625\n",
      "step:136: loss=-0.140625, acc=0.140625\n",
      "step:137: loss=-0.125, acc=0.125\n",
      "step:138: loss=-0.046875, acc=0.046875\n",
      "step:139: loss=-0.078125, acc=0.078125\n",
      "step:140: loss=-0.15625, acc=0.15625\n",
      "step:141: loss=-0.0625, acc=0.0625\n",
      "step:142: loss=-0.0625, acc=0.0625\n",
      "step:143: loss=-0.078125, acc=0.078125\n",
      "step:144: loss=-0.140625, acc=0.140625\n",
      "step:145: loss=-0.15625, acc=0.15625\n",
      "step:146: loss=-0.078125, acc=0.078125\n",
      "step:147: loss=-0.109375, acc=0.109375\n",
      "step:148: loss=-0.109375, acc=0.109375\n",
      "step:149: loss=-0.09375, acc=0.09375\n",
      "step:150: loss=-0.109375, acc=0.109375\n",
      "step:151: loss=-0.1875, acc=0.1875\n",
      "step:152: loss=-0.109375, acc=0.109375\n",
      "step:153: loss=-0.15625, acc=0.15625\n",
      "step:154: loss=-0.15625, acc=0.15625\n",
      "step:155: loss=-0.109375, acc=0.109375\n",
      "step:156: loss=-0.125, acc=0.125\n",
      "step:157: loss=-0.203125, acc=0.203125\n",
      "step:158: loss=-0.125, acc=0.125\n",
      "step:159: loss=-0.15625, acc=0.15625\n",
      "step:160: loss=-0.125, acc=0.125\n",
      "step:161: loss=-0.109375, acc=0.109375\n",
      "step:162: loss=-0.09375, acc=0.09375\n",
      "step:163: loss=-0.078125, acc=0.078125\n",
      "step:164: loss=-0.109375, acc=0.109375\n",
      "step:165: loss=-0.09375, acc=0.09375\n",
      "step:166: loss=-0.125, acc=0.125\n",
      "step:167: loss=-0.125, acc=0.125\n",
      "step:168: loss=-0.140625, acc=0.140625\n",
      "step:169: loss=-0.203125, acc=0.203125\n",
      "step:170: loss=-0.015625, acc=0.015625\n",
      "step:171: loss=-0.0625, acc=0.0625\n",
      "step:172: loss=-0.15625, acc=0.15625\n",
      "step:173: loss=-0.125, acc=0.125\n",
      "step:174: loss=-0.140625, acc=0.140625\n",
      "step:175: loss=-0.125, acc=0.125\n",
      "step:176: loss=-0.078125, acc=0.078125\n",
      "step:177: loss=-0.109375, acc=0.109375\n",
      "step:178: loss=-0.109375, acc=0.109375\n",
      "step:179: loss=-0.09375, acc=0.09375\n",
      "step:180: loss=-0.109375, acc=0.109375\n",
      "step:181: loss=-0.09375, acc=0.09375\n",
      "step:182: loss=-0.09375, acc=0.09375\n",
      "step:183: loss=-0.046875, acc=0.046875\n",
      "step:184: loss=-0.046875, acc=0.046875\n",
      "step:185: loss=-0.125, acc=0.125\n",
      "step:186: loss=-0.109375, acc=0.109375\n",
      "step:187: loss=-0.109375, acc=0.109375\n",
      "step:188: loss=-0.171875, acc=0.171875\n",
      "step:189: loss=-0.0625, acc=0.0625\n",
      "step:190: loss=-0.109375, acc=0.109375\n",
      "step:191: loss=-0.140625, acc=0.140625\n",
      "step:192: loss=-0.09375, acc=0.09375\n",
      "step:193: loss=-0.1875, acc=0.1875\n",
      "step:194: loss=-0.109375, acc=0.109375\n",
      "step:195: loss=-0.140625, acc=0.140625\n",
      "step:196: loss=-0.15625, acc=0.15625\n",
      "step:197: loss=-0.125, acc=0.125\n",
      "step:198: loss=-0.21875, acc=0.21875\n",
      "step:199: loss=-0.125, acc=0.125\n",
      "step:200: loss=-0.078125, acc=0.078125\n",
      "step:201: loss=-0.140625, acc=0.140625\n",
      "step:202: loss=-0.109375, acc=0.109375\n",
      "step:203: loss=-0.09375, acc=0.09375\n",
      "step:204: loss=-0.171875, acc=0.171875\n",
      "step:205: loss=-0.078125, acc=0.078125\n",
      "step:206: loss=-0.15625, acc=0.15625\n",
      "step:207: loss=-0.03125, acc=0.03125\n",
      "step:208: loss=-0.125, acc=0.125\n",
      "step:209: loss=-0.046875, acc=0.046875\n",
      "step:210: loss=-0.109375, acc=0.109375\n",
      "step:211: loss=-0.125, acc=0.125\n",
      "step:212: loss=-0.109375, acc=0.109375\n",
      "step:213: loss=-0.171875, acc=0.171875\n",
      "step:214: loss=-0.1875, acc=0.1875\n",
      "step:215: loss=-0.140625, acc=0.140625\n",
      "step:216: loss=-0.0625, acc=0.0625\n",
      "step:217: loss=-0.09375, acc=0.09375\n",
      "step:218: loss=-0.171875, acc=0.171875\n",
      "step:219: loss=-0.078125, acc=0.078125\n",
      "step:220: loss=-0.109375, acc=0.109375\n",
      "step:221: loss=-0.0625, acc=0.0625\n",
      "step:222: loss=-0.1875, acc=0.1875\n",
      "step:223: loss=-0.15625, acc=0.15625\n",
      "step:224: loss=-0.109375, acc=0.109375\n",
      "step:225: loss=-0.078125, acc=0.078125\n",
      "step:226: loss=-0.15625, acc=0.15625\n",
      "step:227: loss=-0.171875, acc=0.171875\n",
      "step:228: loss=-0.078125, acc=0.078125\n",
      "step:229: loss=-0.0625, acc=0.0625\n",
      "step:230: loss=-0.09375, acc=0.09375\n",
      "step:231: loss=-0.125, acc=0.125\n",
      "step:232: loss=-0.109375, acc=0.109375\n",
      "step:233: loss=-0.15625, acc=0.15625\n",
      "step:234: loss=-0.1875, acc=0.1875\n",
      "step:235: loss=-0.09375, acc=0.09375\n",
      "step:236: loss=-0.109375, acc=0.109375\n",
      "step:237: loss=-0.109375, acc=0.109375\n",
      "step:238: loss=-0.078125, acc=0.078125\n",
      "step:239: loss=-0.140625, acc=0.140625\n",
      "step:240: loss=-0.140625, acc=0.140625\n",
      "step:241: loss=-0.0625, acc=0.0625\n",
      "step:242: loss=-0.109375, acc=0.109375\n",
      "step:243: loss=-0.078125, acc=0.078125\n",
      "step:244: loss=-0.125, acc=0.125\n",
      "step:245: loss=-0.09375, acc=0.09375\n",
      "step:246: loss=-0.0625, acc=0.0625\n",
      "step:247: loss=-0.125, acc=0.125\n",
      "step:248: loss=-0.078125, acc=0.078125\n",
      "step:249: loss=-0.171875, acc=0.171875\n",
      "step:250: loss=-0.09375, acc=0.09375\n",
      "step:251: loss=-0.078125, acc=0.078125\n",
      "step:252: loss=-0.09375, acc=0.09375\n",
      "step:253: loss=-0.15625, acc=0.15625\n",
      "step:254: loss=-0.171875, acc=0.171875\n",
      "step:255: loss=-0.171875, acc=0.171875\n",
      "step:256: loss=-0.09375, acc=0.09375\n",
      "step:257: loss=-0.15625, acc=0.15625\n",
      "step:258: loss=-0.0625, acc=0.0625\n",
      "step:259: loss=-0.125, acc=0.125\n",
      "step:260: loss=-0.109375, acc=0.109375\n",
      "step:261: loss=-0.125, acc=0.125\n",
      "step:262: loss=-0.140625, acc=0.140625\n",
      "step:263: loss=-0.234375, acc=0.234375\n",
      "step:264: loss=-0.09375, acc=0.09375\n",
      "step:265: loss=-0.078125, acc=0.078125\n",
      "step:266: loss=-0.109375, acc=0.109375\n",
      "step:267: loss=-0.109375, acc=0.109375\n",
      "step:268: loss=-0.109375, acc=0.109375\n",
      "step:269: loss=-0.109375, acc=0.109375\n",
      "step:270: loss=-0.109375, acc=0.109375\n",
      "step:271: loss=-0.09375, acc=0.09375\n",
      "step:272: loss=-0.140625, acc=0.140625\n",
      "step:273: loss=-0.109375, acc=0.109375\n",
      "step:274: loss=-0.125, acc=0.125\n",
      "step:275: loss=-0.09375, acc=0.09375\n",
      "step:276: loss=-0.125, acc=0.125\n",
      "step:277: loss=-0.09375, acc=0.09375\n",
      "step:278: loss=-0.09375, acc=0.09375\n",
      "step:279: loss=-0.109375, acc=0.109375\n",
      "step:280: loss=-0.125, acc=0.125\n",
      "step:281: loss=-0.078125, acc=0.078125\n",
      "step:282: loss=-0.09375, acc=0.09375\n",
      "step:283: loss=-0.1875, acc=0.1875\n",
      "step:284: loss=-0.140625, acc=0.140625\n",
      "step:285: loss=-0.09375, acc=0.09375\n",
      "step:286: loss=-0.125, acc=0.125\n",
      "step:287: loss=-0.140625, acc=0.140625\n",
      "step:288: loss=-0.09375, acc=0.09375\n",
      "step:289: loss=-0.109375, acc=0.109375\n",
      "step:290: loss=-0.140625, acc=0.140625\n",
      "step:291: loss=-0.203125, acc=0.203125\n",
      "step:292: loss=-0.09375, acc=0.09375\n",
      "step:293: loss=-0.125, acc=0.125\n",
      "step:294: loss=-0.140625, acc=0.140625\n",
      "step:295: loss=-0.203125, acc=0.203125\n",
      "step:296: loss=-0.09375, acc=0.09375\n",
      "step:297: loss=-0.09375, acc=0.09375\n",
      "step:298: loss=-0.171875, acc=0.171875\n",
      "step:299: loss=-0.125, acc=0.125\n",
      "step:300: loss=-0.140625, acc=0.140625\n",
      "step:301: loss=-0.09375, acc=0.09375\n",
      "step:302: loss=-0.078125, acc=0.078125\n",
      "step:303: loss=-0.09375, acc=0.09375\n",
      "step:304: loss=-0.125, acc=0.125\n",
      "step:305: loss=-0.109375, acc=0.109375\n",
      "step:306: loss=-0.046875, acc=0.046875\n",
      "step:307: loss=-0.015625, acc=0.015625\n",
      "step:308: loss=-0.09375, acc=0.09375\n",
      "step:309: loss=-0.125, acc=0.125\n",
      "step:310: loss=-0.09375, acc=0.09375\n",
      "step:311: loss=-0.109375, acc=0.109375\n",
      "step:312: loss=-0.109375, acc=0.109375\n",
      "step:313: loss=-0.203125, acc=0.203125\n",
      "step:314: loss=-0.15625, acc=0.15625\n",
      "step:315: loss=-0.125, acc=0.125\n",
      "step:316: loss=-0.125, acc=0.125\n",
      "step:317: loss=-0.0625, acc=0.0625\n",
      "step:318: loss=-0.15625, acc=0.15625\n",
      "step:319: loss=-0.109375, acc=0.109375\n",
      "step:320: loss=-0.078125, acc=0.078125\n",
      "step:321: loss=-0.1875, acc=0.1875\n",
      "step:322: loss=-0.125, acc=0.125\n",
      "step:323: loss=-0.078125, acc=0.078125\n",
      "step:324: loss=-0.15625, acc=0.15625\n",
      "step:325: loss=-0.109375, acc=0.109375\n",
      "step:326: loss=-0.0625, acc=0.0625\n",
      "step:327: loss=-0.140625, acc=0.140625\n",
      "step:328: loss=-0.109375, acc=0.109375\n",
      "step:329: loss=-0.09375, acc=0.09375\n",
      "step:330: loss=-0.140625, acc=0.140625\n",
      "step:331: loss=-0.109375, acc=0.109375\n",
      "step:332: loss=-0.0625, acc=0.0625\n",
      "step:333: loss=-0.09375, acc=0.09375\n",
      "step:334: loss=-0.09375, acc=0.09375\n",
      "step:335: loss=-0.125, acc=0.125\n",
      "step:336: loss=-0.140625, acc=0.140625\n",
      "step:337: loss=-0.078125, acc=0.078125\n",
      "step:338: loss=-0.15625, acc=0.15625\n",
      "step:339: loss=-0.09375, acc=0.09375\n",
      "step:340: loss=-0.125, acc=0.125\n",
      "step:341: loss=-0.125, acc=0.125\n",
      "step:342: loss=-0.109375, acc=0.109375\n",
      "step:343: loss=-0.15625, acc=0.15625\n",
      "step:344: loss=-0.109375, acc=0.109375\n",
      "step:345: loss=-0.140625, acc=0.140625\n",
      "step:346: loss=-0.078125, acc=0.078125\n",
      "step:347: loss=-0.125, acc=0.125\n",
      "step:348: loss=-0.140625, acc=0.140625\n",
      "step:349: loss=-0.171875, acc=0.171875\n",
      "step:350: loss=-0.109375, acc=0.109375\n",
      "step:351: loss=-0.15625, acc=0.15625\n",
      "step:352: loss=-0.078125, acc=0.078125\n",
      "step:353: loss=-0.109375, acc=0.109375\n",
      "step:354: loss=-0.15625, acc=0.15625\n",
      "step:355: loss=-0.1875, acc=0.1875\n",
      "step:356: loss=-0.046875, acc=0.046875\n",
      "step:357: loss=-0.171875, acc=0.171875\n",
      "step:358: loss=-0.109375, acc=0.109375\n",
      "step:359: loss=-0.125, acc=0.125\n",
      "step:360: loss=-0.171875, acc=0.171875\n",
      "step:361: loss=-0.078125, acc=0.078125\n",
      "step:362: loss=-0.109375, acc=0.109375\n",
      "step:363: loss=-0.140625, acc=0.140625\n",
      "step:364: loss=-0.078125, acc=0.078125\n",
      "step:365: loss=-0.109375, acc=0.109375\n",
      "step:366: loss=-0.09375, acc=0.09375\n",
      "step:367: loss=-0.09375, acc=0.09375\n",
      "step:368: loss=-0.046875, acc=0.046875\n",
      "step:369: loss=-0.0625, acc=0.0625\n",
      "step:370: loss=-0.078125, acc=0.078125\n",
      "step:371: loss=-0.109375, acc=0.109375\n",
      "step:372: loss=-0.125, acc=0.125\n",
      "step:373: loss=-0.109375, acc=0.109375\n",
      "step:374: loss=-0.046875, acc=0.046875\n",
      "step:375: loss=-0.09375, acc=0.09375\n",
      "step:376: loss=-0.140625, acc=0.140625\n",
      "step:377: loss=-0.1875, acc=0.1875\n",
      "step:378: loss=-0.140625, acc=0.140625\n",
      "step:379: loss=-0.078125, acc=0.078125\n",
      "step:380: loss=-0.171875, acc=0.171875\n",
      "step:381: loss=-0.1875, acc=0.1875\n",
      "step:382: loss=-0.078125, acc=0.078125\n",
      "step:383: loss=-0.046875, acc=0.046875\n",
      "step:384: loss=-0.140625, acc=0.140625\n",
      "step:385: loss=-0.171875, acc=0.171875\n",
      "step:386: loss=-0.125, acc=0.125\n",
      "step:387: loss=-0.125, acc=0.125\n",
      "step:388: loss=-0.078125, acc=0.078125\n",
      "step:389: loss=-0.125, acc=0.125\n",
      "step:390: loss=-0.125, acc=0.125\n",
      "step:391: loss=-0.09375, acc=0.09375\n",
      "step:392: loss=-0.140625, acc=0.140625\n",
      "step:393: loss=-0.109375, acc=0.109375\n",
      "step:394: loss=-0.140625, acc=0.140625\n",
      "step:395: loss=-0.09375, acc=0.09375\n",
      "step:396: loss=-0.09375, acc=0.09375\n",
      "step:397: loss=-0.125, acc=0.125\n",
      "step:398: loss=-0.046875, acc=0.046875\n",
      "step:399: loss=-0.171875, acc=0.171875\n",
      "step:400: loss=-0.109375, acc=0.109375\n",
      "step:401: loss=-0.109375, acc=0.109375\n",
      "step:402: loss=-0.0625, acc=0.0625\n",
      "step:403: loss=-0.171875, acc=0.171875\n",
      "step:404: loss=-0.1875, acc=0.1875\n",
      "step:405: loss=-0.15625, acc=0.15625\n",
      "step:406: loss=-0.15625, acc=0.15625\n",
      "step:407: loss=-0.0625, acc=0.0625\n",
      "step:408: loss=-0.234375, acc=0.234375\n",
      "step:409: loss=-0.125, acc=0.125\n",
      "step:410: loss=-0.140625, acc=0.140625\n",
      "step:411: loss=-0.078125, acc=0.078125\n",
      "step:412: loss=-0.109375, acc=0.109375\n",
      "step:413: loss=-0.140625, acc=0.140625\n",
      "step:414: loss=-0.1875, acc=0.1875\n",
      "step:415: loss=-0.03125, acc=0.03125\n",
      "step:416: loss=-0.09375, acc=0.09375\n",
      "step:417: loss=-0.140625, acc=0.140625\n",
      "step:418: loss=-0.125, acc=0.125\n",
      "step:419: loss=-0.125, acc=0.125\n",
      "step:420: loss=-0.09375, acc=0.09375\n",
      "step:421: loss=-0.109375, acc=0.109375\n",
      "step:422: loss=-0.0625, acc=0.0625\n",
      "step:423: loss=-0.140625, acc=0.140625\n",
      "step:424: loss=-0.078125, acc=0.078125\n",
      "step:425: loss=-0.046875, acc=0.046875\n",
      "step:426: loss=-0.09375, acc=0.09375\n",
      "step:427: loss=-0.125, acc=0.125\n",
      "step:428: loss=-0.1875, acc=0.1875\n",
      "step:429: loss=-0.109375, acc=0.109375\n",
      "step:430: loss=-0.171875, acc=0.171875\n",
      "step:431: loss=-0.09375, acc=0.09375\n",
      "step:432: loss=-0.09375, acc=0.09375\n",
      "step:433: loss=-0.078125, acc=0.078125\n",
      "step:434: loss=-0.125, acc=0.125\n",
      "step:435: loss=-0.078125, acc=0.078125\n",
      "step:436: loss=-0.109375, acc=0.109375\n",
      "step:437: loss=-0.171875, acc=0.171875\n",
      "step:438: loss=-0.046875, acc=0.046875\n",
      "step:439: loss=-0.109375, acc=0.109375\n",
      "step:440: loss=-0.109375, acc=0.109375\n",
      "step:441: loss=-0.09375, acc=0.09375\n",
      "step:442: loss=-0.1875, acc=0.1875\n",
      "step:443: loss=-0.15625, acc=0.15625\n",
      "step:444: loss=-0.0625, acc=0.0625\n",
      "step:445: loss=-0.25, acc=0.25\n",
      "step:446: loss=-0.109375, acc=0.109375\n",
      "step:447: loss=-0.109375, acc=0.109375\n",
      "step:448: loss=-0.125, acc=0.125\n",
      "step:449: loss=-0.0625, acc=0.0625\n",
      "step:450: loss=-0.109375, acc=0.109375\n",
      "step:451: loss=-0.125, acc=0.125\n",
      "step:452: loss=-0.15625, acc=0.15625\n",
      "step:453: loss=-0.140625, acc=0.140625\n",
      "step:454: loss=-0.046875, acc=0.046875\n",
      "step:455: loss=-0.140625, acc=0.140625\n",
      "step:456: loss=-0.15625, acc=0.15625\n",
      "step:457: loss=-0.046875, acc=0.046875\n",
      "step:458: loss=-0.109375, acc=0.109375\n",
      "step:459: loss=-0.09375, acc=0.09375\n",
      "step:460: loss=-0.15625, acc=0.15625\n",
      "step:461: loss=-0.09375, acc=0.09375\n",
      "step:462: loss=-0.0625, acc=0.0625\n",
      "step:463: loss=-0.09375, acc=0.09375\n",
      "step:464: loss=-0.125, acc=0.125\n",
      "step:465: loss=-0.09375, acc=0.09375\n",
      "step:466: loss=-0.125, acc=0.125\n",
      "step:467: loss=-0.15625, acc=0.15625\n",
      "step:468: loss=-0.203125, acc=0.203125\n",
      "step:469: loss=-0.15625, acc=0.15625\n",
      "step:470: loss=-0.125, acc=0.125\n",
      "step:471: loss=-0.078125, acc=0.078125\n",
      "step:472: loss=-0.078125, acc=0.078125\n",
      "step:473: loss=-0.125, acc=0.125\n",
      "step:474: loss=-0.125, acc=0.125\n",
      "step:475: loss=-0.09375, acc=0.09375\n",
      "step:476: loss=-0.140625, acc=0.140625\n",
      "step:477: loss=-0.15625, acc=0.15625\n",
      "step:478: loss=-0.171875, acc=0.171875\n",
      "step:479: loss=-0.140625, acc=0.140625\n",
      "step:480: loss=-0.140625, acc=0.140625\n",
      "step:481: loss=-0.125, acc=0.125\n",
      "step:482: loss=-0.125, acc=0.125\n",
      "step:483: loss=-0.125, acc=0.125\n",
      "step:484: loss=-0.078125, acc=0.078125\n",
      "step:485: loss=-0.0625, acc=0.0625\n",
      "step:486: loss=-0.09375, acc=0.09375\n",
      "step:487: loss=-0.0625, acc=0.0625\n",
      "step:488: loss=-0.046875, acc=0.046875\n",
      "step:489: loss=-0.109375, acc=0.109375\n",
      "step:490: loss=-0.125, acc=0.125\n",
      "step:491: loss=-0.125, acc=0.125\n",
      "step:492: loss=-0.046875, acc=0.046875\n",
      "step:493: loss=-0.140625, acc=0.140625\n",
      "step:494: loss=-0.15625, acc=0.15625\n",
      "step:495: loss=-0.078125, acc=0.078125\n",
      "step:496: loss=-0.015625, acc=0.015625\n",
      "step:497: loss=-0.046875, acc=0.046875\n",
      "step:498: loss=-0.171875, acc=0.171875\n",
      "step:499: loss=-0.15625, acc=0.15625\n",
      "step:500: loss=-0.09375, acc=0.09375\n",
      "step:501: loss=-0.15625, acc=0.15625\n",
      "step:502: loss=-0.03125, acc=0.03125\n",
      "step:503: loss=-0.09375, acc=0.09375\n",
      "step:504: loss=-0.109375, acc=0.109375\n",
      "step:505: loss=-0.140625, acc=0.140625\n",
      "step:506: loss=-0.109375, acc=0.109375\n",
      "step:507: loss=-0.109375, acc=0.109375\n",
      "step:508: loss=-0.09375, acc=0.09375\n",
      "step:509: loss=-0.109375, acc=0.109375\n",
      "step:510: loss=-0.09375, acc=0.09375\n",
      "step:511: loss=-0.125, acc=0.125\n",
      "step:512: loss=-0.09375, acc=0.09375\n",
      "step:513: loss=-0.140625, acc=0.140625\n",
      "step:514: loss=-0.125, acc=0.125\n",
      "step:515: loss=-0.109375, acc=0.109375\n",
      "step:516: loss=-0.125, acc=0.125\n",
      "step:517: loss=-0.0625, acc=0.0625\n",
      "step:518: loss=-0.109375, acc=0.109375\n",
      "step:519: loss=-0.078125, acc=0.078125\n",
      "step:520: loss=-0.140625, acc=0.140625\n",
      "step:521: loss=-0.109375, acc=0.109375\n",
      "step:522: loss=-0.140625, acc=0.140625\n",
      "step:523: loss=-0.109375, acc=0.109375\n",
      "step:524: loss=-0.09375, acc=0.09375\n",
      "step:525: loss=-0.078125, acc=0.078125\n",
      "step:526: loss=-0.09375, acc=0.09375\n",
      "step:527: loss=-0.125, acc=0.125\n",
      "step:528: loss=-0.078125, acc=0.078125\n",
      "step:529: loss=-0.125, acc=0.125\n",
      "step:530: loss=-0.09375, acc=0.09375\n",
      "step:531: loss=-0.109375, acc=0.109375\n",
      "step:532: loss=-0.09375, acc=0.09375\n",
      "step:533: loss=-0.078125, acc=0.078125\n",
      "step:534: loss=-0.078125, acc=0.078125\n",
      "step:535: loss=-0.046875, acc=0.046875\n",
      "step:536: loss=-0.078125, acc=0.078125\n",
      "step:537: loss=-0.15625, acc=0.15625\n",
      "step:538: loss=-0.078125, acc=0.078125\n",
      "step:539: loss=-0.03125, acc=0.03125\n",
      "step:540: loss=-0.109375, acc=0.109375\n",
      "step:541: loss=-0.078125, acc=0.078125\n",
      "step:542: loss=-0.03125, acc=0.03125\n",
      "step:543: loss=-0.125, acc=0.125\n",
      "step:544: loss=-0.0625, acc=0.0625\n",
      "step:545: loss=-0.125, acc=0.125\n",
      "step:546: loss=-0.046875, acc=0.046875\n",
      "step:547: loss=-0.125, acc=0.125\n",
      "step:548: loss=-0.09375, acc=0.09375\n",
      "step:549: loss=-0.15625, acc=0.15625\n",
      "step:550: loss=-0.078125, acc=0.078125\n",
      "step:551: loss=-0.109375, acc=0.109375\n",
      "step:552: loss=-0.0625, acc=0.0625\n",
      "step:553: loss=-0.0625, acc=0.0625\n",
      "step:554: loss=-0.109375, acc=0.109375\n",
      "step:555: loss=-0.125, acc=0.125\n",
      "step:556: loss=-0.109375, acc=0.109375\n",
      "step:557: loss=-0.109375, acc=0.109375\n",
      "step:558: loss=-0.09375, acc=0.09375\n",
      "step:559: loss=-0.046875, acc=0.046875\n",
      "step:560: loss=-0.125, acc=0.125\n",
      "step:561: loss=-0.046875, acc=0.046875\n",
      "step:562: loss=-0.078125, acc=0.078125\n",
      "step:563: loss=-0.140625, acc=0.140625\n",
      "step:564: loss=-0.15625, acc=0.15625\n",
      "step:565: loss=-0.109375, acc=0.109375\n",
      "step:566: loss=-0.09375, acc=0.09375\n",
      "step:567: loss=-0.046875, acc=0.046875\n",
      "step:568: loss=-0.203125, acc=0.203125\n",
      "step:569: loss=-0.046875, acc=0.046875\n",
      "step:570: loss=-0.0625, acc=0.0625\n",
      "step:571: loss=-0.078125, acc=0.078125\n",
      "step:572: loss=-0.15625, acc=0.15625\n",
      "step:573: loss=-0.125, acc=0.125\n",
      "step:574: loss=-0.09375, acc=0.09375\n",
      "step:575: loss=-0.078125, acc=0.078125\n",
      "step:576: loss=-0.140625, acc=0.140625\n",
      "step:577: loss=-0.125, acc=0.125\n",
      "step:578: loss=-0.0625, acc=0.0625\n",
      "step:579: loss=-0.1875, acc=0.1875\n",
      "step:580: loss=-0.203125, acc=0.203125\n",
      "step:581: loss=-0.15625, acc=0.15625\n",
      "step:582: loss=-0.078125, acc=0.078125\n",
      "step:583: loss=-0.109375, acc=0.109375\n",
      "step:584: loss=-0.125, acc=0.125\n",
      "step:585: loss=-0.109375, acc=0.109375\n",
      "step:586: loss=-0.09375, acc=0.09375\n",
      "step:587: loss=-0.03125, acc=0.03125\n",
      "step:588: loss=-0.109375, acc=0.109375\n",
      "step:589: loss=-0.15625, acc=0.15625\n",
      "step:590: loss=-0.09375, acc=0.09375\n",
      "step:591: loss=-0.109375, acc=0.109375\n",
      "step:592: loss=-0.078125, acc=0.078125\n",
      "step:593: loss=-0.09375, acc=0.09375\n",
      "step:594: loss=-0.109375, acc=0.109375\n",
      "step:595: loss=-0.0625, acc=0.0625\n",
      "step:596: loss=-0.09375, acc=0.09375\n",
      "step:597: loss=-0.125, acc=0.125\n",
      "step:598: loss=-0.03125, acc=0.03125\n",
      "step:599: loss=-0.140625, acc=0.140625\n",
      "step:600: loss=-0.078125, acc=0.078125\n",
      "step:601: loss=-0.09375, acc=0.09375\n",
      "step:602: loss=-0.03125, acc=0.03125\n",
      "step:603: loss=-0.171875, acc=0.171875\n",
      "step:604: loss=-0.109375, acc=0.109375\n",
      "step:605: loss=-0.109375, acc=0.109375\n",
      "step:606: loss=-0.203125, acc=0.203125\n",
      "step:607: loss=-0.125, acc=0.125\n",
      "step:608: loss=-0.125, acc=0.125\n",
      "step:609: loss=-0.078125, acc=0.078125\n",
      "step:610: loss=-0.109375, acc=0.109375\n",
      "step:611: loss=-0.125, acc=0.125\n",
      "step:612: loss=-0.03125, acc=0.03125\n",
      "step:613: loss=-0.09375, acc=0.09375\n",
      "step:614: loss=-0.078125, acc=0.078125\n",
      "step:615: loss=-0.109375, acc=0.109375\n",
      "step:616: loss=-0.109375, acc=0.109375\n",
      "step:617: loss=-0.09375, acc=0.09375\n",
      "step:618: loss=-0.109375, acc=0.109375\n",
      "step:619: loss=-0.109375, acc=0.109375\n",
      "step:620: loss=-0.078125, acc=0.078125\n",
      "step:621: loss=-0.078125, acc=0.078125\n",
      "step:622: loss=-0.1875, acc=0.1875\n",
      "step:623: loss=-0.171875, acc=0.171875\n",
      "step:624: loss=-0.046875, acc=0.046875\n",
      "step:625: loss=-0.0625, acc=0.0625\n",
      "step:626: loss=-0.109375, acc=0.109375\n",
      "step:627: loss=-0.109375, acc=0.109375\n",
      "step:628: loss=-0.03125, acc=0.03125\n",
      "step:629: loss=-0.109375, acc=0.109375\n",
      "step:630: loss=-0.09375, acc=0.09375\n",
      "step:631: loss=-0.078125, acc=0.078125\n",
      "step:632: loss=-0.21875, acc=0.21875\n",
      "step:633: loss=-0.109375, acc=0.109375\n",
      "step:634: loss=-0.171875, acc=0.171875\n",
      "step:635: loss=-0.03125, acc=0.03125\n",
      "step:636: loss=-0.078125, acc=0.078125\n",
      "step:637: loss=-0.125, acc=0.125\n",
      "step:638: loss=-0.09375, acc=0.09375\n",
      "step:639: loss=-0.109375, acc=0.109375\n",
      "step:640: loss=-0.09375, acc=0.09375\n",
      "step:641: loss=-0.0625, acc=0.0625\n",
      "step:642: loss=-0.140625, acc=0.140625\n",
      "step:643: loss=-0.09375, acc=0.09375\n",
      "step:644: loss=-0.03125, acc=0.03125\n",
      "step:645: loss=-0.078125, acc=0.078125\n",
      "step:646: loss=-0.125, acc=0.125\n",
      "step:647: loss=-0.125, acc=0.125\n",
      "step:648: loss=-0.0625, acc=0.0625\n",
      "step:649: loss=-0.140625, acc=0.140625\n",
      "step:650: loss=-0.078125, acc=0.078125\n",
      "step:651: loss=-0.09375, acc=0.09375\n",
      "step:652: loss=-0.0625, acc=0.0625\n",
      "step:653: loss=-0.09375, acc=0.09375\n",
      "step:654: loss=-0.09375, acc=0.09375\n",
      "step:655: loss=-0.109375, acc=0.109375\n",
      "step:656: loss=-0.046875, acc=0.046875\n",
      "step:657: loss=-0.140625, acc=0.140625\n",
      "step:658: loss=-0.109375, acc=0.109375\n",
      "step:659: loss=-0.140625, acc=0.140625\n",
      "step:660: loss=-0.078125, acc=0.078125\n",
      "step:661: loss=-0.078125, acc=0.078125\n",
      "step:662: loss=-0.0625, acc=0.0625\n",
      "step:663: loss=-0.0625, acc=0.0625\n",
      "step:664: loss=-0.09375, acc=0.09375\n",
      "step:665: loss=-0.09375, acc=0.09375\n",
      "step:666: loss=-0.140625, acc=0.140625\n",
      "step:667: loss=-0.0625, acc=0.0625\n",
      "step:668: loss=-0.140625, acc=0.140625\n",
      "step:669: loss=-0.1875, acc=0.1875\n",
      "step:670: loss=-0.109375, acc=0.109375\n",
      "step:671: loss=-0.125, acc=0.125\n",
      "step:672: loss=-0.109375, acc=0.109375\n",
      "step:673: loss=-0.09375, acc=0.09375\n",
      "step:674: loss=-0.046875, acc=0.046875\n",
      "step:675: loss=-0.078125, acc=0.078125\n",
      "step:676: loss=-0.15625, acc=0.15625\n",
      "step:677: loss=-0.0625, acc=0.0625\n",
      "step:678: loss=-0.15625, acc=0.15625\n",
      "step:679: loss=-0.125, acc=0.125\n",
      "step:680: loss=-0.125, acc=0.125\n",
      "step:681: loss=-0.09375, acc=0.09375\n",
      "step:682: loss=-0.140625, acc=0.140625\n",
      "step:683: loss=-0.140625, acc=0.140625\n",
      "step:684: loss=-0.09375, acc=0.09375\n",
      "step:685: loss=-0.171875, acc=0.171875\n",
      "step:686: loss=-0.046875, acc=0.046875\n",
      "step:687: loss=-0.125, acc=0.125\n",
      "step:688: loss=-0.046875, acc=0.046875\n",
      "step:689: loss=-0.171875, acc=0.171875\n",
      "step:690: loss=-0.09375, acc=0.09375\n",
      "step:691: loss=-0.203125, acc=0.203125\n",
      "step:692: loss=-0.15625, acc=0.15625\n",
      "step:693: loss=-0.1875, acc=0.1875\n",
      "step:694: loss=-0.125, acc=0.125\n",
      "step:695: loss=-0.09375, acc=0.09375\n",
      "step:696: loss=-0.109375, acc=0.109375\n",
      "step:697: loss=-0.09375, acc=0.09375\n",
      "step:698: loss=-0.1875, acc=0.1875\n",
      "step:699: loss=-0.140625, acc=0.140625\n",
      "step:700: loss=-0.0625, acc=0.0625\n",
      "step:701: loss=-0.078125, acc=0.078125\n",
      "step:702: loss=-0.203125, acc=0.203125\n",
      "step:703: loss=-0.140625, acc=0.140625\n",
      "step:704: loss=-0.15625, acc=0.15625\n",
      "step:705: loss=-0.078125, acc=0.078125\n",
      "step:706: loss=-0.109375, acc=0.109375\n",
      "step:707: loss=-0.125, acc=0.125\n",
      "step:708: loss=-0.046875, acc=0.046875\n",
      "step:709: loss=-0.109375, acc=0.109375\n",
      "step:710: loss=-0.078125, acc=0.078125\n",
      "step:711: loss=-0.15625, acc=0.15625\n",
      "step:712: loss=-0.078125, acc=0.078125\n",
      "step:713: loss=-0.171875, acc=0.171875\n",
      "step:714: loss=-0.125, acc=0.125\n",
      "step:715: loss=-0.0625, acc=0.0625\n",
      "step:716: loss=-0.046875, acc=0.046875\n",
      "step:717: loss=-0.078125, acc=0.078125\n",
      "step:718: loss=-0.046875, acc=0.046875\n",
      "step:719: loss=-0.0625, acc=0.0625\n",
      "step:720: loss=-0.15625, acc=0.15625\n",
      "step:721: loss=-0.09375, acc=0.09375\n",
      "step:722: loss=-0.15625, acc=0.15625\n",
      "step:723: loss=-0.109375, acc=0.109375\n",
      "step:724: loss=-0.0625, acc=0.0625\n",
      "step:725: loss=-0.125, acc=0.125\n",
      "step:726: loss=-0.125, acc=0.125\n",
      "step:727: loss=-0.15625, acc=0.15625\n",
      "step:728: loss=-0.09375, acc=0.09375\n",
      "step:729: loss=-0.09375, acc=0.09375\n",
      "step:730: loss=-0.125, acc=0.125\n",
      "step:731: loss=-0.1875, acc=0.1875\n",
      "step:732: loss=-0.078125, acc=0.078125\n",
      "step:733: loss=-0.078125, acc=0.078125\n",
      "step:734: loss=-0.15625, acc=0.15625\n",
      "step:735: loss=-0.109375, acc=0.109375\n",
      "step:736: loss=-0.109375, acc=0.109375\n",
      "step:737: loss=-0.125, acc=0.125\n",
      "step:738: loss=-0.09375, acc=0.09375\n",
      "step:739: loss=-0.171875, acc=0.171875\n",
      "step:740: loss=-0.078125, acc=0.078125\n",
      "step:741: loss=-0.234375, acc=0.234375\n",
      "step:742: loss=-0.140625, acc=0.140625\n",
      "step:743: loss=-0.125, acc=0.125\n",
      "step:744: loss=-0.109375, acc=0.109375\n",
      "step:745: loss=-0.09375, acc=0.09375\n",
      "step:746: loss=-0.125, acc=0.125\n",
      "step:747: loss=-0.15625, acc=0.15625\n",
      "step:748: loss=-0.046875, acc=0.046875\n",
      "step:749: loss=-0.078125, acc=0.078125\n",
      "step:750: loss=-0.109375, acc=0.109375\n",
      "step:751: loss=-0.171875, acc=0.171875\n",
      "step:752: loss=-0.15625, acc=0.15625\n",
      "step:753: loss=-0.140625, acc=0.140625\n",
      "step:754: loss=-0.171875, acc=0.171875\n",
      "step:755: loss=-0.09375, acc=0.09375\n",
      "step:756: loss=-0.109375, acc=0.109375\n",
      "step:757: loss=-0.109375, acc=0.109375\n",
      "step:758: loss=-0.078125, acc=0.078125\n",
      "step:759: loss=-0.09375, acc=0.09375\n",
      "step:760: loss=-0.125, acc=0.125\n",
      "step:761: loss=-0.125, acc=0.125\n",
      "step:762: loss=-0.125, acc=0.125\n",
      "step:763: loss=-0.140625, acc=0.140625\n",
      "step:764: loss=-0.171875, acc=0.171875\n",
      "step:765: loss=-0.125, acc=0.125\n",
      "step:766: loss=-0.125, acc=0.125\n",
      "step:767: loss=-0.078125, acc=0.078125\n",
      "step:768: loss=-0.140625, acc=0.140625\n",
      "step:769: loss=-0.078125, acc=0.078125\n",
      "step:770: loss=-0.0625, acc=0.0625\n",
      "step:771: loss=-0.15625, acc=0.15625\n",
      "step:772: loss=-0.21875, acc=0.21875\n",
      "step:773: loss=-0.03125, acc=0.03125\n",
      "step:774: loss=-0.203125, acc=0.203125\n",
      "step:775: loss=-0.125, acc=0.125\n",
      "step:776: loss=-0.078125, acc=0.078125\n",
      "step:777: loss=-0.0625, acc=0.0625\n",
      "step:778: loss=-0.03125, acc=0.03125\n",
      "step:779: loss=-0.0625, acc=0.0625\n",
      "step:780: loss=-0.078125, acc=0.078125\n",
      "step:781: loss=-0.109375, acc=0.109375\n",
      "step:782: loss=-0.09375, acc=0.09375\n",
      "step:783: loss=-0.078125, acc=0.078125\n",
      "step:784: loss=-0.078125, acc=0.078125\n",
      "step:785: loss=-0.078125, acc=0.078125\n",
      "step:786: loss=-0.09375, acc=0.09375\n",
      "step:787: loss=-0.09375, acc=0.09375\n",
      "step:788: loss=-0.078125, acc=0.078125\n",
      "step:789: loss=-0.171875, acc=0.171875\n",
      "step:790: loss=-0.0625, acc=0.0625\n",
      "step:791: loss=-0.03125, acc=0.03125\n",
      "step:792: loss=-0.0625, acc=0.0625\n",
      "step:793: loss=-0.171875, acc=0.171875\n",
      "step:794: loss=-0.109375, acc=0.109375\n",
      "step:795: loss=-0.140625, acc=0.140625\n",
      "step:796: loss=-0.125, acc=0.125\n",
      "step:797: loss=-0.140625, acc=0.140625\n",
      "step:798: loss=-0.078125, acc=0.078125\n",
      "step:799: loss=-0.140625, acc=0.140625\n",
      "step:800: loss=-0.140625, acc=0.140625\n",
      "step:801: loss=-0.109375, acc=0.109375\n",
      "step:802: loss=-0.125, acc=0.125\n",
      "step:803: loss=-0.078125, acc=0.078125\n",
      "step:804: loss=-0.125, acc=0.125\n",
      "step:805: loss=-0.0625, acc=0.0625\n",
      "step:806: loss=-0.125, acc=0.125\n",
      "step:807: loss=-0.140625, acc=0.140625\n",
      "step:808: loss=-0.078125, acc=0.078125\n",
      "step:809: loss=-0.078125, acc=0.078125\n",
      "step:810: loss=-0.078125, acc=0.078125\n",
      "step:811: loss=-0.0625, acc=0.0625\n",
      "step:812: loss=-0.078125, acc=0.078125\n",
      "step:813: loss=-0.125, acc=0.125\n",
      "step:814: loss=-0.140625, acc=0.140625\n",
      "step:815: loss=-0.09375, acc=0.09375\n",
      "step:816: loss=-0.109375, acc=0.109375\n",
      "step:817: loss=-0.0625, acc=0.0625\n",
      "step:818: loss=-0.15625, acc=0.15625\n",
      "step:819: loss=-0.09375, acc=0.09375\n",
      "step:820: loss=-0.15625, acc=0.15625\n",
      "step:821: loss=-0.03125, acc=0.03125\n",
      "step:822: loss=-0.109375, acc=0.109375\n",
      "step:823: loss=-0.109375, acc=0.109375\n",
      "step:824: loss=-0.078125, acc=0.078125\n",
      "step:825: loss=-0.0625, acc=0.0625\n",
      "step:826: loss=-0.1875, acc=0.1875\n",
      "step:827: loss=-0.15625, acc=0.15625\n",
      "step:828: loss=-0.125, acc=0.125\n",
      "step:829: loss=-0.109375, acc=0.109375\n",
      "step:830: loss=-0.109375, acc=0.109375\n",
      "step:831: loss=-0.140625, acc=0.140625\n",
      "step:832: loss=-0.078125, acc=0.078125\n",
      "step:833: loss=-0.09375, acc=0.09375\n",
      "step:834: loss=-0.078125, acc=0.078125\n",
      "step:835: loss=-0.109375, acc=0.109375\n",
      "step:836: loss=-0.140625, acc=0.140625\n",
      "step:837: loss=-0.15625, acc=0.15625\n",
      "step:838: loss=-0.078125, acc=0.078125\n",
      "step:839: loss=-0.09375, acc=0.09375\n",
      "step:840: loss=-0.09375, acc=0.09375\n",
      "step:841: loss=-0.078125, acc=0.078125\n",
      "step:842: loss=-0.0625, acc=0.0625\n",
      "step:843: loss=-0.09375, acc=0.09375\n",
      "step:844: loss=-0.0625, acc=0.0625\n",
      "step:845: loss=-0.171875, acc=0.171875\n",
      "step:846: loss=-0.140625, acc=0.140625\n",
      "step:847: loss=-0.125, acc=0.125\n",
      "step:848: loss=-0.078125, acc=0.078125\n",
      "step:849: loss=-0.03125, acc=0.03125\n",
      "step:850: loss=-0.140625, acc=0.140625\n",
      "step:851: loss=-0.125, acc=0.125\n",
      "step:852: loss=-0.125, acc=0.125\n",
      "step:853: loss=-0.109375, acc=0.109375\n",
      "step:854: loss=-0.125, acc=0.125\n",
      "step:855: loss=-0.0625, acc=0.0625\n",
      "step:856: loss=-0.09375, acc=0.09375\n",
      "step:857: loss=-0.046875, acc=0.046875\n",
      "step:858: loss=-0.171875, acc=0.171875\n",
      "step:859: loss=-0.0625, acc=0.0625\n",
      "step:860: loss=-0.125, acc=0.125\n",
      "step:861: loss=-0.09375, acc=0.09375\n",
      "step:862: loss=-0.15625, acc=0.15625\n",
      "step:863: loss=-0.15625, acc=0.15625\n",
      "step:864: loss=-0.171875, acc=0.171875\n",
      "step:865: loss=-0.125, acc=0.125\n",
      "step:866: loss=-0.078125, acc=0.078125\n",
      "step:867: loss=-0.171875, acc=0.171875\n",
      "step:868: loss=-0.09375, acc=0.09375\n",
      "step:869: loss=-0.0625, acc=0.0625\n",
      "step:870: loss=-0.171875, acc=0.171875\n",
      "step:871: loss=-0.0625, acc=0.0625\n",
      "step:872: loss=-0.0625, acc=0.0625\n",
      "step:873: loss=-0.0625, acc=0.0625\n",
      "step:874: loss=-0.15625, acc=0.15625\n",
      "step:875: loss=-0.15625, acc=0.15625\n",
      "step:876: loss=-0.109375, acc=0.109375\n",
      "step:877: loss=-0.078125, acc=0.078125\n",
      "step:878: loss=-0.125, acc=0.125\n",
      "step:879: loss=-0.140625, acc=0.140625\n",
      "step:880: loss=-0.109375, acc=0.109375\n",
      "step:881: loss=-0.078125, acc=0.078125\n",
      "step:882: loss=-0.140625, acc=0.140625\n",
      "step:883: loss=-0.078125, acc=0.078125\n",
      "step:884: loss=-0.109375, acc=0.109375\n",
      "step:885: loss=-0.140625, acc=0.140625\n",
      "step:886: loss=-0.15625, acc=0.15625\n",
      "step:887: loss=-0.140625, acc=0.140625\n",
      "step:888: loss=-0.09375, acc=0.09375\n",
      "step:889: loss=-0.0625, acc=0.0625\n",
      "step:890: loss=-0.1875, acc=0.1875\n",
      "step:891: loss=-0.09375, acc=0.09375\n",
      "step:892: loss=-0.0625, acc=0.0625\n",
      "step:893: loss=-0.125, acc=0.125\n",
      "step:894: loss=-0.109375, acc=0.109375\n",
      "step:895: loss=-0.15625, acc=0.15625\n",
      "step:896: loss=-0.125, acc=0.125\n",
      "step:897: loss=-0.203125, acc=0.203125\n",
      "step:898: loss=-0.15625, acc=0.15625\n",
      "step:899: loss=-0.078125, acc=0.078125\n",
      "step:900: loss=-0.109375, acc=0.109375\n",
      "step:901: loss=-0.078125, acc=0.078125\n",
      "step:902: loss=-0.09375, acc=0.09375\n",
      "step:903: loss=-0.109375, acc=0.109375\n",
      "step:904: loss=-0.125, acc=0.125\n",
      "step:905: loss=-0.109375, acc=0.109375\n",
      "step:906: loss=-0.140625, acc=0.140625\n",
      "step:907: loss=-0.09375, acc=0.09375\n",
      "step:908: loss=-0.1875, acc=0.1875\n",
      "step:909: loss=-0.125, acc=0.125\n",
      "step:910: loss=-0.078125, acc=0.078125\n",
      "step:911: loss=-0.0625, acc=0.0625\n",
      "step:912: loss=-0.09375, acc=0.09375\n",
      "step:913: loss=-0.046875, acc=0.046875\n",
      "step:914: loss=-0.1875, acc=0.1875\n",
      "step:915: loss=-0.046875, acc=0.046875\n",
      "step:916: loss=-0.125, acc=0.125\n",
      "step:917: loss=-0.1875, acc=0.1875\n",
      "step:918: loss=-0.171875, acc=0.171875\n",
      "step:919: loss=-0.09375, acc=0.09375\n",
      "step:920: loss=-0.109375, acc=0.109375\n",
      "step:921: loss=-0.109375, acc=0.109375\n",
      "step:922: loss=-0.15625, acc=0.15625\n",
      "step:923: loss=-0.109375, acc=0.109375\n",
      "step:924: loss=-0.125, acc=0.125\n",
      "step:925: loss=-0.0625, acc=0.0625\n",
      "step:926: loss=-0.1875, acc=0.1875\n",
      "step:927: loss=-0.140625, acc=0.140625\n",
      "step:928: loss=-0.125, acc=0.125\n",
      "step:929: loss=-0.09375, acc=0.09375\n",
      "step:930: loss=-0.09375, acc=0.09375\n",
      "step:931: loss=-0.15625, acc=0.15625\n",
      "step:932: loss=-0.078125, acc=0.078125\n",
      "step:933: loss=-0.1875, acc=0.1875\n",
      "step:934: loss=-0.0625, acc=0.0625\n",
      "step:935: loss=-0.171875, acc=0.171875\n",
      "step:936: loss=-0.078125, acc=0.078125\n",
      "step:937: loss=-0.125, acc=0.125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [86]\u001B[0m, in \u001B[0;36m<cell line: 43>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     58\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmean(epoch_loss))\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m test_data, test_target \u001B[38;5;129;01min\u001B[39;00m test:\n\u001B[1;32m---> 60\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze(test_data)\n\u001B[0;32m     61\u001B[0m     pred \u001B[38;5;241m=\u001B[39m model(test_data\u001B[38;5;241m.\u001B[39mcuda())\n\u001B[0;32m     62\u001B[0m     test_target \u001B[38;5;241m=\u001B[39m test_target\n",
      "Input \u001B[1;32mIn [86]\u001B[0m, in \u001B[0;36m<cell line: 43>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     58\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmean(epoch_loss))\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m test_data, test_target \u001B[38;5;129;01min\u001B[39;00m test:\n\u001B[1;32m---> 60\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze(test_data)\n\u001B[0;32m     61\u001B[0m     pred \u001B[38;5;241m=\u001B[39m model(test_data\u001B[38;5;241m.\u001B[39mcuda())\n\u001B[0;32m     62\u001B[0m     test_target \u001B[38;5;241m=\u001B[39m test_target\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\221.6008.17\\plugins\\python\\helpers\\pydev\\pydevd.py:1155\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1152\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\221.6008.17\\plugins\\python\\helpers\\pydev\\pydevd.py:1170\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1167\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1169\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1170\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import Sequential\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 64\n",
    "max_epoch = 100\n",
    "dataloader = DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "# test_data = torch.FloatTensor(test_data).cuda()\n",
    "# test_target = torch.LongTensor(test_target).cuda()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = Sequential(\n",
    "            nn.Linear(784, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 10, bias=True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.cuda()\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    for step, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_x = torch.squeeze(batch_x)\n",
    "        batch_y = batch_y.cuda()\n",
    "        pred = model(batch_x)\n",
    "        loss = loss_func(pred, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = torch.eq(pred.argmax(1), batch_y).sum() / len(batch_y)\n",
    "        epoch_acc.append(acc.cpu().data)\n",
    "        print(f\"step:{step}: loss={loss.cpu().data}, acc={acc}\")\n",
    "    train_loss.append(np.mean(epoch_loss))\n",
    "    for test_data, test_target in test:\n",
    "        test_data = torch.squeeze(test_data)\n",
    "        pred = model(test_data.cuda())\n",
    "        test_target = test_target\n",
    "        loss = loss_func(pred, test_target)\n",
    "        acc = torch.eq(pred.argmax(1), test_target).sum() / len(test_target)\n",
    "        test_loss.append(loss.cpu().data)\n",
    "        print(\n",
    "            f\"epoch {epoch}: train_loss: {np.mean(epoch_loss)}, test_loss: {loss.cpu().data}, train_acc={np.mean(epoch_acc) * 100}, test_acc={acc * 100}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}